# Testing Guide for Trailhead Backend

## Overview

This document outlines the testing requirements, best practices, and procedures for the Trailhead backend application. **All backend features must include comprehensive unit tests to ensure code quality and prevent regressions.**

## Testing Requirements

### Mandatory Testing Policy

**⚠️ IMPORTANT: All new backend features MUST include unit tests before being merged.**

When adding any new feature to the backend, you must:

1. Write comprehensive unit tests covering all endpoints and business logic
2. Ensure tests pass locally against the PostgreSQL database
3. Achieve at least 80% code coverage for new code
4. Update this document if new testing patterns or fixtures are introduced

### What Must Be Tested

Every new backend feature requires tests for:

- ✅ **API Endpoints**: All HTTP methods (GET, POST, PUT, PATCH, DELETE)
- ✅ **Authentication/Authorization**: Both authenticated and unauthenticated access
- ✅ **Input Validation**: Valid and invalid data, edge cases
- ✅ **Error Handling**: Error responses and status codes
- ✅ **Database Operations**: CRUD operations through API and directly through CRUD functions
- ✅ **Business Logic**: All validation rules and complex logic
- ✅ **Relationships**: Cascade deletes, foreign key constraints
- ✅ **Edge Cases**: Empty lists, non-existent resources, permission boundaries

## Test Environment Setup

### Database Configuration

Tests use the **actual PostgreSQL database** running in the Docker container. This ensures tests run against the same database engine as production.

#### Configuration

The test suite automatically reads database credentials from `credentials.txt` in the project root:

```python
# credentials.txt
PostgreSQL User: trailhead
PostgreSQL Password: <generated-password>
PostgreSQL Database: trailhead
PostgreSQL Port: 5432
```

**Prerequisites:**

- Docker containers must be running (`docker-compose up -d`)
- Database must be accessible on `localhost:5432`
- `credentials.txt` must exist (generated by `bootstrap.sh`)

### Running Tests

#### Run All Tests

```bash
cd backend
python -m pytest
```

#### Run Specific Test File

```bash
python -m pytest tests/test_api_outings.py
```

#### Run Specific Test Class or Function

```bash
python -m pytest tests/test_api_outings.py::TestCreateOuting
python -m pytest tests/test_api_outings.py::TestCreateOuting::test_create_outing_success
```

#### Run with Coverage Report

```bash
python -m pytest --cov=app --cov-report=html --cov-report=term-missing
```

#### View Coverage Report

```bash
open htmlcov/index.html
```

## Test Structure

### Directory Organization

```
backend/tests/
├── conftest.py              # Shared fixtures and configuration
├── factories.py             # Data factory functions for creating test data
├── test_api_*.py           # API endpoint tests
├── test_crud_*.py          # CRUD operation tests
├── test_models.py          # Model tests
└── test_*.py               # Other unit tests
```

### Naming Conventions

- **Test Files**: `test_<module_name>.py` (e.g., `test_api_outings.py`)
- **Test Classes**: `Test<Feature>` (e.g., `TestCreateOuting`)
- **Test Functions**: `test_<action>_<scenario>` (e.g., `test_create_outing_success`)

### Test Organization Pattern

```python
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
class TestFeatureName:
    """Test <specific feature> endpoint/functionality"""

    async def test_feature_success(self, client: AsyncClient, test_user):
        """Test successful <feature> operation"""
        # Arrange
        data = {"field": "value"}

        # Act
        response = await client.post("/api/endpoint", json=data)

        # Assert
        assert response.status_code == 200
        assert response.json()["field"] == "value"

    async def test_feature_validation_error(self, client: AsyncClient):
        """Test <feature> with invalid data"""
        response = await client.post("/api/endpoint", json={})
        assert response.status_code == 422

    async def test_feature_not_found(self, client: AsyncClient):
        """Test <feature> with non-existent resource"""
        response = await client.get("/api/endpoint/nonexistent-id")
        assert response.status_code == 404

    async def test_feature_unauthorized(self, client: AsyncClient):
        """Test <feature> without authentication"""
        response = await client.post("/api/endpoint", json={})
        assert response.status_code == 403
```

## Available Fixtures

### Database Fixtures

- `db_session`: AsyncSession for database operations
- `client`: Unauthenticated HTTP test client
- `authenticated_client`: HTTP client with mocked admin authentication

### User Fixtures

- `test_user`: Admin user
- `test_regular_user`: Regular (non-admin) user
- `test_user_token`: JWT token for test_user
- `test_regular_user_token`: JWT token for test_regular_user
- `auth_headers`: Authorization headers with admin token
- `regular_user_headers`: Authorization headers with regular user token

### Model Fixtures

- `test_outing`: Test outing (overnight)
- `test_day_outing`: Test day outing (not overnight)
- `test_signup`: Test signup with participants
- `test_place`: Test place/location
- `test_family_member`: Test family member
- `test_rank_requirement`: Test rank requirement
- `test_merit_badge`: Test merit badge

### Data Fixtures

- `sample_outing_data`: Dictionary with sample outing data
- `sample_signup_data`: Dictionary with sample signup data

## Authentication in Tests

The application uses Clerk for authentication in production, but tests mock the authentication using FastAPI's dependency override system:

```python
async def test_protected_endpoint(self, client: AsyncClient, test_user):
    """Test accessing a protected endpoint"""
    from app.main import app
    from app.api import deps

    # Mock authentication
    async def override_get_current_user():
        return test_user

    app.dependency_overrides[deps.get_current_user] = override_get_current_user

    try:
        response = await client.get(
            "/api/protected",
            headers={"Authorization": "Bearer mock_token"}
        )
        assert response.status_code == 200
    finally:
        app.dependency_overrides.clear()
```

For admin-only endpoints:

```python
app.dependency_overrides[deps.get_current_admin_user] = override_get_current_admin
```

## Test Coverage Requirements

### Minimum Coverage Standards

- **Overall Coverage**: Minimum 80% for all application code
- **New Features**: Minimum 90% for newly added code
- **Critical Paths**: 100% for authentication, authorization, and payment processing

### Checking Coverage

```bash
# Generate coverage report
python -m pytest --cov=app --cov-report=term-missing

# Generate HTML report for detailed view
python -m pytest --cov=app --cov-report=html
open htmlcov/index.html
```

### Coverage Exclusions

The following are excluded from coverage requirements:

- Migration files
- `__init__.py` files with only imports
- Development/debug utilities
- Third-party integrations (Clerk, etc.)

## Writing Good Tests

### Best Practices

1. **Test One Thing**: Each test should verify one specific behavior
2. **Descriptive Names**: Test names should clearly describe what they test
3. **Arrange-Act-Assert**: Structure tests with clear setup, execution, and verification
4. **Independence**: Tests should not depend on each other
5. **Clean Database**: Use fixtures to ensure clean state for each test
6. **Meaningful Assertions**: Use specific assertions, not just status codes

### Example: Comprehensive Endpoint Testing

```python
@pytest.mark.asyncio
class TestCreateResource:
    """Test POST /api/resources endpoint"""

    async def test_create_success(self, client: AsyncClient, test_user):
        """Test successful resource creation"""
        # This tests the happy path
        data = {"name": "Test Resource", "value": 100}
        response = await client.post("/api/resources", json=data)
        assert response.status_code == 201
        assert response.json()["name"] == "Test Resource"

    async def test_create_missing_required_fields(self, client: AsyncClient):
        """Test creation with missing required fields"""
        # This tests validation
        response = await client.post("/api/resources", json={})
        assert response.status_code == 422

    async def test_create_invalid_data_type(self, client: AsyncClient):
        """Test creation with invalid data types"""
        # This tests type validation
        data = {"name": "Test", "value": "not_a_number"}
        response = await client.post("/api/resources", json=data)
        assert response.status_code == 422

    async def test_create_duplicate(self, client: AsyncClient, existing_resource):
        """Test creation with duplicate data"""
        # This tests uniqueness constraints
        data = {"name": existing_resource.name}
        response = await client.post("/api/resources", json=data)
        assert response.status_code == 400
        assert "already exists" in response.json()["detail"].lower()

    async def test_create_unauthorized(self, client: AsyncClient):
        """Test creation without authentication"""
        # This tests authorization
        response = await client.post("/api/resources", json={"name": "Test"})
        assert response.status_code == 403
```

## Common Testing Patterns

### Testing CRUD Operations

```python
# CREATE
async def test_create(self, client, auth_headers):
    response = await client.post("/api/resource", headers=auth_headers, json=data)
    assert response.status_code == 201

# READ (single)
async def test_get(self, client, auth_headers, test_resource):
    response = await client.get(f"/api/resource/{test_resource.id}", headers=auth_headers)
    assert response.status_code == 200

# READ (list)
async def test_list(self, client, auth_headers):
    response = await client.get("/api/resource", headers=auth_headers)
    assert response.status_code == 200
    assert isinstance(response.json(), list)

# UPDATE
async def test_update(self, client, auth_headers, test_resource):
    response = await client.put(
        f"/api/resource/{test_resource.id}",
        headers=auth_headers,
        json={"name": "Updated"}
    )
    assert response.status_code == 200

# DELETE
async def test_delete(self, client, auth_headers, test_resource):
    response = await client.delete(f"/api/resource/{test_resource.id}", headers=auth_headers)
    assert response.status_code == 204
```

### Testing Relationships

```python
async def test_cascade_delete(self, client, auth_headers, test_parent):
    """Test that deleting parent deletes children"""
    # Create child
    child_response = await client.post(
        "/api/children",
        headers=auth_headers,
        json={"parent_id": str(test_parent.id), "name": "Child"}
    )
    child_id = child_response.json()["id"]

    # Delete parent
    await client.delete(f"/api/parents/{test_parent.id}", headers=auth_headers)

    # Verify child is also deleted
    response = await client.get(f"/api/children/{child_id}", headers=auth_headers)
    assert response.status_code == 404
```

## Continuous Integration

### Pre-Commit Checks

Before committing code:

```bash
# Run all tests
python -m pytest

# Check coverage
python -m pytest --cov=app --cov-report=term-missing --cov-fail-under=80
```

### GitHub Actions (Future)

Tests will automatically run on:

- Pull requests
- Pushes to `main` branch
- Scheduled nightly builds

## Troubleshooting

### Common Issues

#### Database Connection Errors

```
psycopg.OperationalError: connection failed
```

**Solution**: Ensure Docker containers are running:

```bash
docker-compose up -d
docker-compose ps  # Verify postgres is running
```

#### Tests Interfering With Each Other

**Solution**: Tests share the same database. If tests are not properly isolated:

- Use transactions that rollback
- Clear data in fixtures
- Use unique identifiers (UUIDs)

#### Slow Tests

**Solution**:

- Run specific test files instead of full suite during development
- Use `pytest -x` to stop on first failure
- Consider marking slow tests with `@pytest.mark.slow` and skip them during development

## Adding a New Feature - Testing Checklist

When adding a new backend feature, complete this checklist:

- [ ] Create test file `tests/test_api_<feature>.py`
- [ ] Add necessary fixtures to `conftest.py` or test file
- [ ] Test all CRUD operations (if applicable)
- [ ] Test authentication requirements
- [ ] Test authorization (admin vs regular user)
- [ ] Test input validation (valid and invalid data)
- [ ] Test error cases (not found, conflicts, etc.)
- [ ] Test relationships and cascade operations
- [ ] Test edge cases (empty lists, null values, etc.)
- [ ] Run coverage report and ensure >80% for new code
- [ ] Update this TESTING.md if new patterns are introduced

## Resources

- [pytest documentation](https://docs.pytest.org/)
- [pytest-asyncio documentation](https://pytest-asyncio.readthedocs.io/)
- [FastAPI testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [SQLAlchemy testing](https://docs.sqlalchemy.org/en/20/orm/session_transaction.html#joining-a-session-into-an-external-transaction)

## Questions or Issues?

If you encounter issues with tests or have questions about testing practices, please:

1. Check this document first
2. Review existing test files for examples
3. Ask the team in Slack #development channel

---

**Remember: Tests are not optional. They are a critical part of feature development and help ensure the quality and reliability of the Trailhead application.**
